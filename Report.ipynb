{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Study: Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\"Independent Study: Introduction to Deep Learning\" is an independent study class conducted by Krystal Ly and advised by Dr. Anthony Bonifonte. The main focus of the class is to explore Deep Learning through readings, online courses, and a central project. Throughout the course, we have a chance to apply data analysis skills including, but not limited to, web scraping, data wrangling, data visualization, data modeling, and so on. In addition, we explore the basic of neural network, back propagation, different activation functions, and so on. This report serves as a comprehensive summary of what have been done in the class. \n",
    "\n",
    "The primary goal of this independent study is to develop a foundational understanding of deep learning, combine data analysis skills and the new knowledge to work on an exploratory project, explore the interpretability of neural network models in deep learning, and explore the different choices being made in the process of building a deep learning model.\n",
    "\n",
    "Part B of the report gives an overview of the project (the methodology and data being used). Part C summarizes the data collection, exploratory data analysis, and data wrangling process. The feature engineering and modeling part are then explained in part D. The last part then gives a conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Overview of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a project conducted throughout the course of this class to facilitate learning. The project aims at using a dataset of computed book-tag scores in order to predict the different book genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Book Genome Dataset\n",
    "The project uses the Book Genome Dataset. This can be accessed [here](https://grouplens.org/datasets/book-genome/). The Book Genome Dataset contains book-tag scores generated for a set of books along with the data used for its generation (raw data). Book-tag scores indicate the degree to which a tag applies to a book. For example, for the book \"Twilight\", we can have a high \"vampire\" tag score and low \"math\" tag score because the book is more about vampire and almost does not have anything relate to math. These book-tag scores are computed through a complex process using a multilevel nonlinear regression model. The input data involves the tag applications correspond to tags that users attach to books, user reviews, user ratings, and user survey. There are multiple datasets in the Book Genome Dataset repository but these two datasets are used in the project:\n",
    "- `metadata.json`: The file contains information about 9,374 books from Goodreads\n",
    "\n",
    "    |Variable Name|Definition| \n",
    "    |-|-|\n",
    "    |item_id|book id|url – link to the book page at the Goodreads website|\n",
    "    |title|book title (9,348 unique titles)|\n",
    "    |authors|book authors|\n",
    "    |lang|book language|\n",
    "    |img|link to an image of book cover|\n",
    "    |year|book release year|\n",
    "    |description|book description|\n",
    "    \n",
    "&nbsp;\n",
    "- `tagdl.csv`: The file contains 6,814,898 tag genome scores between -0.2 and 1.18 (due to the algorithm design) based on the TagDL algorithm [Kotkov et al., 2021].\n",
    "\n",
    "    |Variable Name|Definition| \n",
    "    |-|-|\n",
    "    |tag|tag string (727 unique tags)|\n",
    "    |item_id|book id (9,374 unique ids)|\n",
    "    |score|degree, with which the tag applies to the book|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Genre Dataset\n",
    "Since the project aims at completing a book genre classification task, it is necessary to get data of the genres. The data is scraped from the Goodreads website using the book's Goodreads link in the `metadata.json`. The `weighted-genre.json` consists of the book id, the genres associated with that book and the number of people who add that genre to that book. \n",
    "\n",
    "|Variable Name|Definition| \n",
    "|-|-|\n",
    "|item_id|book id (9,374 unique ids)|\n",
    "|young-adult|number of users apply the genre young-adult to the book|\n",
    "|romance|number of users apply the genre romance to the book|\n",
    "\n",
    "and more genres..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Collection\n",
    "- Exploratory Data Analysis\n",
    "- Data Wrangling\n",
    "- Modeling\n",
    "- Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data Collection, Data Exploration, and Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This part uses the BeautifulSoup library to build a web scraper. The link to the list of genre is \"https://www.goodreads.com/work/shelves/{book_id}\" so we start with getting the links to all the books' genres web page using the book id. A sample of the web page:\n",
    "\n",
    "![Fig1](images/Fig1.png)\n",
    "Image Source: Goodreads\n",
    "\n",
    "We then use BeautifulSoup to extract the genres, number of people who add that genre to that book and save it in the json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Exploring the `metadata` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` dataset has 9374 observations. Column `description` has 65 missing values, and column `year` has 1256 missing values. However, we do not plan to use these columns so we are not worried about them.\n",
    "\n",
    "Figure 2 shows the distribution of the `year` column. Books in the dataset were released from 1912 to 2017. However, most of the books were released in 2000s and 2010s. \n",
    "\n",
    "![Fig2](images/Fig2.png)\n",
    "\n",
    "Figure 3 shows the distribution of the `lang` column. All the languages are different types of English.\n",
    "\n",
    "![Fig3](images/Fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Exploring the `tags` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tags` dataset has 6,814,898 with no missing values. There are 727 unique tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Steps that we have taken for data wrangling:\n",
    "- Load the `weighted_genre.json` file\n",
    "- Transform it to a pandas dataframe\n",
    "- Remove the extra \",\" and convert the numbers to integer type\n",
    "\n",
    "Next, we have to figure out a way to match the numbers to either 0 or 1 indicating whether a specific book has a specific genre or not. Notes that users can apply as many genre tags for a book as they want. We normalize the genres by books, and only take genres that have a value higher than 0.5 after normalization. It is equivalent to assign 1 to a genre $x$ if\n",
    "\n",
    "$\\frac{\\text{difference between the number of tagged times of the genre x and the least popular genre in the same book}} {\\text{difference between the number of tagged times of the most popular genre and the least popular genre in the same book}} > 0.5$\n",
    "\n",
    "Figure 4 shows the number of books in the top 10 genres. Note that a book can have more than one genre. \n",
    "\n",
    "![Fig4](images/Fig4.png)\n",
    "\n",
    "The top 4 genres are roughly the same so we choose to explore the young adult genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Modeling - Young Adult genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### I. A review of some technial knowledge used in the Modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Early Stopping\n",
    "When training neural network models, there are many choices to make, and these choices can affect the result. One of the most major choices is the choice of the number of epochs, which is the number of iterations we should run on our model. On the one hand, if we train our models on too few iterations, the model may underfit the train and validation sets. On the other hand, if we train our models on too many iterations, the model may overfit the training dataset and hence, have a poor performance on the validation set. \n",
    "\n",
    "One simple, effective, and widely used technique to overcome this problem is early stopping. With early stopping, we stop the training when the performance on the validation set starts to degrade. \n",
    "\n",
    "In our modeling process, we set the maximum number of epochs to be 5000 and stop the training when the performance on the validation loss shows no improvement for 10 consecutive steps.\n",
    "\n",
    "#### 2. McCulloch and Pitts neurons\n",
    "McCulloch and Pitts neurons, also known as M-P neurons, are a type of simplified artificial neuron model. The M-P neuron model was an important early step in the development of neural networks and artificial intelligence. Although it has some limitations and is not widely used today, it was one of the first attempts to model the workings of biological neurons using mathematics and logic. The basic concepts behind the M-P neuron model have influenced the development of more advanced neural network models, such as the perceptron and multi-layer perceptron. \n",
    "\n",
    "Here is a picture of M-P neuron model:\n",
    "\n",
    "![Fig12](images/Fig12.png)\n",
    "\n",
    "Image Source: Chapman Hall- Machine Learning: An Algorithmic Perspective\n",
    "\n",
    "\n",
    "The M-P neuron model is a binary model that takes one or more inputs and produces a binary output. The output of the neuron is determined by whether the weighted sum of the inputs crosses a threshold value or not. If the weighted sum exceeds the threshold, the neuron produces an output of 1, otherwise, it produces an output of 0.\n",
    "\n",
    "The inputs $x_i$ are multiplied by the weights $w_i$, and the neurons sum their values. If this sum is greater than the threshold θ then the neuron fires; otherwise it does not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Activation functions\n",
    "In neural networks, an activation function of a neuron is a mathematical function that is applied to the output of that neuron given an input or set of inputs. The activation function takes the weighted sum of the inputs and biases of a neuron and applies a non-linear transformation to it, producing the output of the neuron. It introduces non-linearity to the model and allows the neural network to learn complex, non-linear relationships between input and output data.\n",
    "\n",
    "There are many types of activation functions that can be used in neural networks. Some of the most popular ones include the sigmoid, tanh, ReLU, LeakyReLU, and softmax. There is no answer to the choice of activation function, and experimenting is one way to see which activation functions work for a particular problem.\n",
    "\n",
    "##### a. Sigmoid function\n",
    "The sigmoid function is a mathematical function that maps any input or set of inputs to a value between 0 and 1. Figure 7 is a graph of the sigmoid function.\n",
    "\n",
    "![Fig7](images/Fig7.png)\n",
    "\n",
    "Image source: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "Sigmoid function is often used in the output layer for a binary classification problem because it guarantees that the output has a range from 0 to 1. In our project, we will always use sigmoid function in the output layer.\n",
    "\n",
    "##### b. Softmax function\n",
    "The softmax function is an activation function that is utilized for multi-classification tasks and is a more generalized form of the logistic activation function.\n",
    "\n",
    "##### c. Tanh function\n",
    "The tanh function is a shifted version of the logistic regression function. It has a range of [-1,1] instead of [0,1]. The tanh function often works better in hidden layers than the logistic regression function. The only drawback is that the tanh function may slow down gradient descent if z is very large or small.\n",
    "\n",
    "![Fig8](images/Fig8.png)\n",
    "\n",
    "Image source: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "##### d. ReLU function\n",
    "The ReLU, or Rectified Linear Unit, is one of the mose used activation function. It outputs the input or linear combination of the set of inputs if the input or linear combination of the set of inputs is greater than 0 and outputs 0 otherwise. \n",
    "\n",
    "The derivative of the ReLU function is 0 when z < 0 and 1 when z > 1. In practice, z is usually greater than 0 so the derivative is usually 1, leading to fast learning.\n",
    "\n",
    "![Fig9](images/Fig9.png)\n",
    "\n",
    "Image Source: https://www.researchgate.net/publication/367096679_Modelling_of_Depth_Prediction_Algorithm_for_Intra_Prediction_Complexity_Reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and machine learning. It transforms a high-dimensional dataset into a lower-dimensional representation while retaining as much information as possible.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works:\n",
    "\n",
    "- Standardize the data: PCA begins by standardizing the dataset to have zero mean and unit variance. This step is important to ensure that each feature contributes equally to the analysis.\n",
    "\n",
    "- Compute the covariance matrix: The covariance matrix is calculated based on the standardized data. It represents the relationships between different variables and one another.\n",
    "\n",
    "- Eigendecomposition: The next step involves finding the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions or axes in the feature space along which the data has the most variance, while eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "- Select principal components: The eigenvectors are sorted in descending order of their corresponding eigenvalues. This ranking reflects the importance of each principal component in capturing the variability in the data. The first principal component captures the most variance in the data, followed by the second, the third, and so on. Typically, only a subset of the principal components is selected, based on the desired amount of variance we want to retain.\n",
    "\n",
    "- Project the data to create a new dataset using PCA: The selected principal components are used to create a new feature subspace. The original high-dimensional data is projected onto this subspace to obtain the lower-dimensional representation. This is done by multiplying the original data matrix by the matrix of eigenvectors, corresponding to the selected principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Trainning Test Val Split\n",
    "For small datasets, a rule of thumb is to put 80% of the data in the training set, 10% in the validation set and 10% in the test set. Therefore, we chose 10% of the data for testing, 10.8% of the data for validation, 79.2% of the data for training (the fraction is due to how the tran_test_split function works).\n",
    "\n",
    "#### 2. Baseline Model\n",
    "The baseline model serves as a baseline to evaluate improvements in our models. The baseline model chooses the most frequent class as the prediction. \n",
    "\n",
    "The validation accuracy is 0.792, and the validation F1 score is 0.\n",
    "\n",
    "#### 3. Scikit-learn Logistic Regression Model\n",
    "Scikit-learn is a popular machine learning library in Python. We run a logistic regression model with scikit-learn to have another baseline model to evaluate improvements in our models. \n",
    "\n",
    "The validation accruracy is 0.961, and the validation F1 score is 0.907. This model is doing significantly better than the baseline model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Perceptron - Logistic Regression\n",
    "\n",
    "The first model is the most basic model with only two layers: the input layer (727 units) and the output layer (1 unit, sigmoid activation function). Below is an illustration of this architecture.\n",
    "\n",
    "![Fig5](images/Fig5.png)\n",
    "\n",
    "Image Source: https://thedatafrog.com/en/articles/logistic-regression/\n",
    "\n",
    "The result is illustrated in Figure 6.\n",
    "\n",
    "![Fig6](images/Fig6.png)\n",
    "\n",
    "The validation accuracy does not increase for about 100 consecutive epochs. In addition, the validation f1 score drops to near 0 and does not increase for about 100 consecutive epochs. Since in these epochs, the validation accuracy is around 0.79 but the validation f1 score is near 0, what happens is the model only predicts the most frequent class in these 100 epochs. It seems like the model has succesfully overcome this local optimum as the validation accuracy and f1 score increases steadily after these 100 epochs.\n",
    "\n",
    "With 4166 epochs, the validation accuracy is 0.961, and the validation F1 score is 0.907. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Multi-layer perceptron - ReLU hidden layer (10 units) + ReLU hidden layer (5 units) \n",
    "\n",
    "Since ReLU activation function makes the model learn faster and it is a go-to function when there is no obvious activation function that suits the problem, we choose to first experiment with input layer + ReLU hidden layer (10 units) + ReLU hidden layer (5 units) + output layer (sigmoid function). The result is indeed a faster learning path. \n",
    "\n",
    "![Fig10](images/Fig10.png)\n",
    "\n",
    "The result of this model follows the same trend as the simple Logistic Regression neural-based model. The validation accuracy is constant for about 40 consecutive epochs. In these epochs, the validation accuracy is around 0.79 but the validation f1 score is near 0. In particular, the model only suffers from the class imbalance problem and predicts the most frequent class in these 40 epochs. Then after these 40 epochs, the model overcomes this local optimum and the validation accuracy and f1 score increase steadily. \n",
    "\n",
    "This new model improves as the model loss converges to a good result more quickly than the simple Logistic Regression neural-based model. With only 365 epochs, the validation accuracy is 0.956, and the validation F1 score is 0.896."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Modeling with Feature Scaling\n",
    "\n",
    "Although the tag scores calculated by the TagDL algorithm theoretically have the range from -0.2 to 1.18, the tag scores in reality have different ranges for different tags. Some tags (features) have the theoretical range but some have significantly smaller range. The differences in range may lead to the model being influenced by a few variables that have a larger range. Therefore, feature scaling may result in the model improvement.\n",
    "\n",
    "For feature scaling, there are two common methods: standardization and normalization. Normalization is a technique used to rescale numeric values in a dataset to fall within a specific range, typically between 0 and 1 or -1 and 1. This is achieved by subtracting the minimum value from each observation and then dividing by the range of the data. On the other hand, standardization involves transforming the data to have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean from each observation and then dividing by the standard deviation of the data. \n",
    "\n",
    "While normalization can rescale all features to a specific range, it is vulnerable to the impact of outliers. In contrast, standardization may not bring all features to a specific range, but it is more robust to outliers, and it tends to result in features with similar ranges. Therefore, in this case, we opt to use standardization as our preferred method for scaling the data.\n",
    "\n",
    "Below is the result of the multi-layer perceptron model (ReLU hidden layer with 10 units + ReLU hidden layer with 5 units) with scaled data:\n",
    "\n",
    "![Fig11](images/Fig11.png)\n",
    "\n",
    "After feature scaling is applied, the model never suffers from the class imbalance problem. This may indicate that the previous models rely heavily on some specific variables, and these specific variables are positively correlated with the most frequent class (class 0). \n",
    "\n",
    "The validation accuracy and f1 score increases dramatically for the first 23 or 24 epochs. Then these scores increases at a much lower rate. The validation loss decreases dramatically for the first 53 or 54 epochs before slowing down. The model converges to a good solution even much faster than the previous models. With only 160 epochs, the validation accuracy is 0.957, and the model f1 score is 0.894. Although the model needs less time to converge to a good solution, the validation accuracy and model f1 score don't improve compared to the previous models. In addition, there might be an overfitting problem as with 160 epochs, the training accuracy is 0.988 and the training F1 score is 0.972. This problem may occur because there is so much noise in the data. Because of the noise, the validation data is too different from the training data. Therefore, applying the same scaling parameters of the training data to the validation data may lead to skewing the data in an unexpected way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Modeling with PCA\n",
    "\n",
    "So far, we have been training models using all 727 features. However, since there are only 9374 observations, the number of features may be too large compared to the number of observations. The models may suffer from the \"curse of dimensionality\", meaning that with more features, more data is needed for the model to capture the trends in data. Too many features with not enough observations can potentially lead to the underfitting or overfitting problem. Therefore, the next step that we take is to use Principal Component Analysis (PCA).\n",
    "\n",
    "The figure below shows the cumulative variance by the number of principal components.\n",
    "\n",
    "![Fig13](images/Fig13.png)\n",
    "\n",
    "Typically, 95% of the variance in our data is explained by 536 prinpical components so we choose to keep this subset of principal components.\n",
    "\n",
    "Below is the result of the multi-layer perceptron model (ReLU hidden layer with 10 units + ReLU hidden layer with 5 units) with the transformed data:\n",
    "\n",
    "![Fig14](images/Fig14.png)\n",
    "\n",
    "\n",
    "After tranforming the data with PCA, the model never suffers from the class imbalance problem just like when we scale the data. This may indicate that there are noises in the original data, while the transformed data using 536 principal components ignore the noises and capture the remaining 95% of the variance in the original data.\n",
    "\n",
    "The validation accuracy and f1 score increases dramatically for approximately the first 50 epochs. Then these scores increases at a much lower rate. The validation loss decreases dramatically for the first 40 epochs before slowing down. With 225 epochs, the validation accuracy is 0.96, and the model f1 score is 0.902. Although the model needs less time to converge to a good solution, the validation accuracy and model f1 score don't improve compared to the previous models. In addition, just like with feature scaling, there might be an overfitting problem as with 225 epochs, the training accuracy is 0.994 and the training F1 score is 0.986. Clearly, the model improves but does not generalizes well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Feature Ranking with RFE using sklearn's Logistic Regression\n",
    "\n",
    "Although PCA can help decreasing the number of features, one drawback of using PCA is the interpretability of the transformed data. Since PCA combines and transforms the original features into new components, it becomes less intuitive to understand the transformed data and interpret the result of the model trained on the transformed data. Therefire, we use a method called recursive feature elimination to get a subset of our original features. \n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n",
    "\n",
    "\n",
    "Here we use the Sklearn's Logistic Regression as the external estimator that can assign weights (coefficients) to features. First, the Logistic Regression model is trained on the initial set of features and the coefficient of each feature is obtained either through any specific attribute or callable. Then, the least important features (coefficients closest to 0) are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Modeling with 536 most important features extracted by sklearn's Logistic Regression\n",
    "\n",
    "![Fig15](images/Fig15.png)\n",
    "\n",
    "With 301 epochs, the validation accuracy is 0.95, and the validation F1 score is 0.881. The model does not improve compared to the previous models. There might be also an overfitting problem when the training accuracy is 0.98 and the training F1 score is 0.951."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Modeling with 200 most important features extracted by sklearn's Logistic Regression\n",
    "\n",
    "![Fig16](images/Fig16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model sufers from the class imbalance problem. The validation accuracy rises rapidly to 0.792 (the percentage of the most frequent class) and does not increase for about 70 consecutive spochs. The validation F1 score drops to near 0 and does not increase for about 70 consecutive epochs. \n",
    "\n",
    "The model does not improve compared to previous models. With 429 epochs, the validation accuracy is 0.96, and the validation F1 score is 0.905. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Feature Ranking with RFE using Random Forest\n",
    "\n",
    "We are interested in seeing the relationship between the number of most important features that the model is trained on and the model performance. Since the Logistic Regression model only captures linear relationship, we use the Random Forest model to extract feature importance and recursively eliminate the least important feature from the model until there is only one feature. The result is the ranking of all features in terms of feature importance.\n",
    "\n",
    "We then run neural network on each subset of features in an increasing order of number of features. For example, we first run the model with one most important features followed by two, three, four, five most important features and so on.\n",
    "\n",
    "The results are as follows:\n",
    "\n",
    "![Fig17](images/Fig17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results fluctuate a lot, we apply a smoothing technique called moving average. Each observation is replaced by the average of 10 observations around it (5 before and 5 after including itself). \n",
    "\n",
    "The results after applying the smoothing technique are as follows:\n",
    "\n",
    "![Fig18](images/Fig18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that as the number of features increase, the validation accuracy and the validation F1 score increase. We also expect that as the number of features increase, the validation loss and the number of epochs decrease. Overall, the number of epochs has a downward trend which is consistent with what we expect. However, the validation accuracy, F1 score, and loss have no trends at all and fluctuate dramatically. At around 130 and 360 number of features, the validation accuracy and F1 score decrease dramatically, and the validation loss increases dramatically. More work is needed to figure out the reason behind this.\n",
    "\n",
    "The most important feature is \"teen\". This makes sense because the task is to classify whether a movie falls into the \"young adult\" genre. With just one feature, the model can achieve a validation accuracy of 0.92, and a validation F1 score of 0.836. This is quite significant since the best result we can get so far is approximately 0.96 in validation accuracy and 0.907 in validation F1 score. The second most important feature is \"adult\", followed by \"young adult\", \"romance\", \"fantasy\", and \"high school\". The least important feature is \"divorce\". These all make sense.\n",
    "\n",
    "One important thing that we notice during this process is that the results of a neural network will be different as we change the order of the features in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### III. Summary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Detailed Description|Number of Epochs|Val Accuracy|Val F1 Score|\n",
    "|-|-|-|-|-|\n",
    "Baseline|Predict the most frequent class|None|0.792|0.0|\n",
    "Sklearn Logistic Regression|Using sklearn built-in model|None|0.961|0.907|\n",
    "Percepton -  Simple Log Reg| Input layer (727 units) and Output layer (1 unit, sigmoid activation function)| 4166 |0.961|0.907|\n",
    "Multi-layer perceptron |Input layer (727 units) + ReLU hidden layer (10 units) + ReLU hidden layer (5 units) + Sigmoid output layer (1 unit)|365|0.956|0.896|\n",
    "Multi-layer perceptron with standardized data|Same architecture as above but data is standardized|160|0.957|0.894|\n",
    "Multi-layer perceptron with transformed data by PCA|Same architecture as above but data is transformed using 536 principal components (explain 95% of the variance)|225|0.96|0.902|\n",
    "Multi-layer perceptron with top 536 most important features| Same architecture as above but with only top 536 most important features recursively extracted using sklearn's Logistic Regression | 301 | 0.95 | 0.881|\n",
    "Multi-layer perceptron with top 200 most important features| Same architecture as above but with only top 200 most important features recursively extracted using sklearn's Logistic Regression | 429 | 0.96 | 0.905|\n",
    "Multi-layer perceptron with top 1 most important feature| Same architecture as above but with only top 1 most important feature recursively extracted using sklearn's Random Forest | 4916 | 0.92 | 0.836|\n",
    "Multi-layer perceptron with top 2 most important features| Same architecture as above but with only top 2 most important features recursively extracted using sklearn's Random Forest | 2283 | 0.923 | 0.836|\n",
    "\n",
    "More on how the model performance changes as we include more important features can be found in the file `rf_selector_result.csv` located in the `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below includes only the top 10 most important features. The ranking of all features can be found in the file `rf_selector_ranking_features.csv` located in the data folder.\n",
    "|Rank|Feature Name|\n",
    "|-|-|\n",
    "|1|teen|\n",
    "|2|adult|\n",
    "|3|young adult|\n",
    "|4|romance|\n",
    "|5|fantasy|\n",
    "|6|high school|\n",
    "|7|children's books|\n",
    "|8|high fantasy|\n",
    "|9|realistic|\n",
    "|10|children|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### I. Limitations\n",
    "As the project is a part of the learning and exploring process, there are many limitations. Firstly, the dataset used in the project has only 9374 observations. This number of observations make the dataset not very suitable for a complicated Deep Learning Project so most of the models in the project are simple. More data is needed to train more complicated model and improve the model performance. Secondly, there are too many features compared to the number of observations. Although some actions have been taken to overcome this, the models do not improve and suffer from the overfitting problem. Thirdly, we haven't taken any actions to figure out why the model performance and the number of most important features used do not correlate. \n",
    "\n",
    "### II. Next Steps\n",
    "1. Explore other genres and train models on them to see if we come across the same problems\n",
    "2. Take actions to figure out why the model performance and the number of most important features used do not correlate\n",
    "3. Explore different neural network architectures \n",
    "4. Gather more data or conduct the project with the movie dataset which has relatively similar features but more observations\n",
    "\n",
    "### III. Conclusion\n",
    "In conclusion, the independent study \"Introduction to Deep Learning\" provided a comprehensive exploration of deep learning concepts and their application in data analysis. Throughout the study, various data analysis skills were utilized, including web scraping, data wrangling, data visualization, and modeling. The primary goal was to develop a foundational understanding of deep learning and apply it to an exploratory project. The report served as a culmination of the acquired knowledge and showcased the ability to work on complex problems in the realm of data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
